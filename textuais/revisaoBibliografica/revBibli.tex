\chapter{Revisão Bibliográfica}\label{cap:rev}

Nesse capítulo será apresentada uma revisão bibliográfica dos principais conceitos usados para o desenvolvimento desse trabalho. Essa revisão visa mostrar tanto o início das pesquisas nessa área quanto os avanços realizados nos últimos anos. Os tópicos estudados nesse trabalho foram:

\begin{itemize}
	\item Estimação de Densidades não-paramétrica;
	\item Discretização;
	\item Avaliadores de Estimação;
\end{itemize}

\section{Estimação de Densidade não-paramétrica}

Para análise de grandes quantidades de dados e/ou conjuntos multidimensionais de dados é necessário recorrer à ferramentas estatísticas, como estimadores paramétricos, estimadores não-paramétricos ou recursos gráficos \cite{scott2015multivariate}. 

A primeira abordagem citada pode ser considerada a mais eficiente, uma vez que pode-se obter os parâmetros da função de densidade utilizando o conhecimento a priori sobre os dados, entretanto, tal método pode levar a erros de estimação caso a função geradora dos dados seja desconhecida e/ou não pertencer às função já parametrizadas na literatura \cite{sheskin2003handbook}. Já a segunda ferramenta é considerada mais flexível, no sentido de que não é estritamente necessário algum conhecimento anterior sobre os dados a serem analisados, contudo essa abordagem leva a um problema denominado de 'Maldição da Otimização', onde existem parâmetros a serem otimizados que necessitam de um conhecimento prévio da distribuição, nesse momento algumas suposições, baseadas na análise dos dados, podem ser feitas. Essa alternativa de unir um conhecimento prévio ao método de estimação não-paramétrico gera uma discussão, entre alguns autores (e.g.  \cite{marascuilo1977nonparametric}), sobre o uso do termo '\textit{assumption freer}' ao invés do termo 'não-paramétrico'. Por fim, os recursos gráficos se apresentam como uma ferramenta extra que pode mostrar o problema de outros pontos de vista, trazer luz e mostrar o inesperado.

No caso desta tese o desenvolvimento se dá no âmbito da estimação não-paramétrica, uma vez que a intenção desse trabalho é contribuir para a otimização da estimação e classificação de dados quando não se tem conhecimento sobre as funções geradoras.

%\subsection{Estimação de Densidade não-paramétrica}

No campo da estimação de densidade não-paramétrica podemos citar o pesquisador David W. Scott por suas inúmeras contribuições: estudos sobre o histograma \cite{histogramScott}, primeiro estimador não-paramétrico, a concepção do \ac{ASH} \cite{ashScott}, pesquisas relacionadas a otimização de histogramas \cite{scott1979optimal}, outros estimadores não-paramétricos \cite{scott1987biased}, \cite{scott1981monte} e \cite{wang19941}, otimizações do \ac{KDE} \cite{terrell1992variable}, \cite{scott1977kernel} e \cite{scott1985kernel} e estudos sobre estimação de densidades multidimensionais \cite{sain1994cross}, muitos desses tópicos abordados de maneira clara no livro \cite{scott2015multivariate}, que serviu como base para o início das pesquisas dessa tese.

Além disso, analisando os estudos desse tema na literatura temos que os métodos de \ac{KDE} datam de \cite{rosenblatt1956remarks} e \cite{parzen1962estimation}, depois desses podemos citar os trabalhos de  \cite{eubank1988spline}, \cite{hardle1990applied}, \cite{hardle1988smoothing}, \cite{silverman1986density}, \cite{wahba1990spline}, \cite{wand1994kernel} e \cite{jones1996brief} como exemplos de estudos relacionados ao tema de suavização da estimação de densidades, tais métodos são, por muitas vezes, capazes de fornecer uma metodologia útil e eficiente para análise dos dados.

O \ac{KDE} é amplamente utilizado devido a sua simplicidade tanto na implementação quanto na interpretação de seus resultados, em \cite{zambom2012review} o autor apresenta uma revisão sobre esse método, mostrando importantes aspectos teóricos do mesmo. O \ac{KDE} apresenta uso em casos de modelagem, inferência estatística e análise de dados, entretanto, sua utilização pode ser restringida devido ao seu custo computacional, quando comparado a outros métodos \cite{tang2016fast}. 

Com esse detalhe em mente e sabendo que, atualmente, muitos experimentos exigem otimização no sentido de custo computacional e precisão, pode-se citar tópicos como: escolha da largura de banda, sensibilidade a descontinuidades e o próprio custo computacional como escopo de estudo para aprimorar a estimação de densidade baseada no \ac{KDE}. Contudo, ressalta-se que para a otimização de cada tópico citado necessita-se de uma análise profunda e individual, mesmo assim, em muitos casos, pode-se chegar a questões de otimização cíclica. Nos próximos parágrafos serão apresentados alguns dos principais trabalhos desenvolvidos sobre cada tema.


\subsection{Custo computacional}

Mesmo com a publicação de muitos livros e artigos sobre o tema de estimação de densidades nas últimas décadas, as soluções e implementações desenvolvidas com o \ac{KDE} ainda consomem muito tempo. Em casos onde o conjunto de dados a ser analisado não passa de algumas centenas esse problema pode ser desconsiderado, entretanto, quando almeja-se avaliar grandes quantidades de dados \textit{(Big Data)}, essa questão pode ser um obstáculo, principalmente em casos multidimensionais \cite{gramacki2017nonparametric}.

Existem muitos estudos que propõem formas de diminuir o consumo de tempo dos algoritmos de \ac{KDE}, sendo que a maioria faz uso de métodos de aproximação computacional, entretanto, o erro dessas aproximações não pode ser controlado. Com isso em mente os autores de \cite{raykar2010fast} apresentam uma ferramenta baseada em expansão de Taylor, mantendo somente os primeiros termos com o intuito de garantir que o erro causado pelo truncamento da serie permaneça menor que o erro desejado. Em \cite{tang2016fast} é proposto um método de estimação unidimensional e multidimensional baseada em polinômios locais e comparado com o método de \ac{KDE} padrão. Já em \cite{langrene2017fast} é apresentado o método baseado em \textit{Fast Sum Updating} que consiste na ordenação dos dados de entrada e na transição do \textit{kernel} de um ponto de avaliação para outro, atualizando somente os pontos de entrada que não pertenciam a janela selecionada anteriormente. Ainda dentro desse mesmo tema temos \cite{yin2008fast} que aborda esse conteúdo utilizando a teoria de \ac{SBR}. E, de modo geral, o trabalho de \cite{lang2005empirical} apresenta resultados de experimentos testando os métodos \ac{FGT}, \ac{IFGT} e Árvore Dupla (do inglês, \textit{Dual-Tree}) em relação ao tamanho do conjunto de dados, dimensão, erro permitido, além de medidas de tempo de CPU e uso de memória.

\subsection{Escolha da Largura de Banda}

A discussão sobre a seleção de largura de banda (do inglês, \textit{Bandwidth}) vem acontecendo a cerca de três décadas \cite{heidenreich2013bandwidth}. Embora existam outros aspectos a serem analisados na estimativa de densidade, a escolha desse parâmetro não é de forma alguma menos problemática. Isso se torna visível ao ler os muitos estudos empíricos em que os autores apresentam novas propostas normalmente fornecendo simulações em que mostram que o próprio seletor supera alguns dos métodos existentes. Entretanto a seleção da largura de banda é algo muito complexo e nada trivial, nenhum dos muitos métodos de seleção existentes conseguem superar os outros quando se considera as mais variadas densidades a serem estimadas , como mostrado em \cite{heidenreich2013bandwidth} e \cite{turlach1993bandwidth}.

A escolha da largura de banda é sensível a muitos aspectos da distribuição, como \textit{Kurtosis} \cite{decarlo1997meaning}, \textit{Skewness} \cite{mardia1970measures}, caudas exponenciais, descontinuidades, multi-picos, tamanho da amostra, entre outros. Neste contexto, pode-se citar os artigos de \cite{oyang2005data} e \cite{kim2012robust}, o primeiro apresenta a técnica de calcular a largura de banda variável com um modelo mais relaxado, o segundo aborda o tema da ponto de vista da estatística robusta, que tem como principal contribuição a não consideração de pontos fora da densidade (\textit{outliers}) para os cálculo estatísticos e de largura de banda. E as teses de doutorado de \cite{walter1998density}, \cite{duong2004bandwidth} e \cite{schindler2012bandwidth}.

\subsection{Sensibilidade a descontinuidades}


Para a estimação de densidades é comum fazer uso de medidas estatísticas baseadas em momentos. No entanto, quando não é verdadeira a suposição de que a distribuição em questão é uma distribuição Gaussiana, os modelos calculados a partir dessa medida não são ótimos. Um dos casos em que isso pode acontecer é quando os dados contêm pontos distantes ou discrepantes dos parâmetros medidos \textit{outliers}. No sentido estatístico, os \textit{outliers} são amostras de uma população diferente da maioria dos dados. A presença dessas amostras pode ser devido a dois motivos principais: erro experimental ou característica única de alguns objetos. Ambos os tipos de \textit{outliers} são importantes para serem identificados, no entanto, a razão para a sua identificação é diferente em cada caso, no primeiro deseja-se removê-los dos dados para obter resultados corretos da análise, já no segundo, encontrar a explicação para a sua ocorrência no intuito de entender melhor o processo estudado \cite{daszykowski2007robust}.

O tema de identificação de \textit{outliers} e estudo sobre a sensibilidade dos estimadores a esses eventos, recebeu o nome de Estatística Robusta, é vastamente discutido em muitos artigos, como por exemplo: \cite{huber19721972} que faz um revisão dos métodos de estatística robusta; \cite{analytical1989robust} que explora o ponto controverso de que os \textit{outliers} em alguns casos precisam ser considerados e não completamente rejeitados; \cite{rousseeuw2018computation} que aborda o tema de estatística robusta no mundo multivariado; entre outros. Além disso, alguns livros, que podem ser usados como base nesse estudo, foram publicados ao longo dos anos, pode-se citar \cite{rousseeuw2005robust}, \cite{maronna2006robust}, \cite{hampel2011robust} e \cite{olive2018robust}.

É de conhecimento da literatura que um único \textit{outlier} nos dados pode alterar completamente a tendência geral e tornar um modelo inválido para a maioria dos dados. Portanto, faz-se necessário o uso de estimadores robustos em conjunto com abordagens clássicas para melhorar a estimação de densidades e minimizar o erros causados por \textit{outliers}.

\section{Discretização}

A estimativa de densidade via KDE de um conjunto de medidas contínuas, por razões computacionais, é frequentemente representada de forma discreta. Consequentemente, a estimativa direta só acontece para os valores discretos \cite{jones1989discretized} e a interpolação é usada para calcular qualquer outro valor que possa surgir durante as medições. Esse processo insere um erro de estimativa que pode ser minimizado pelo aumento do número de pontos estimados, criando um \textit{trade-off} entre otimização computacional e desempenho de estimativa.

Muitos autores seguem a mesma abordagem de \cite{jones1989discretized}, explorando os diferentes aspectos do processo de discretização e propondo novos métodos para minimizar as adversidades reveladas. Por exemplo, em \cite{fayyad1993multi} é proposto o método Ent-MDLP; em \cite{friedman1996discretizing}, um algoritmo de discretização baseado em \textit{Bayesian Networks} é apresentado; Em \cite{biba2007unsupervised}, os autores propuseram um método não supervisionado de discretização usando o KDE; também utilizando um método não supervisionado, os autores de \cite{schmidberger2005unsupervised} apresentam o estudo da discretização aplicada à estimação da densidade baseada em \textit{Tree-Based}; e em \cite{zhang2007discretization} foi estudado um algoritmo de aprendizado de máquina baseado no critério \textit{Gini}.

Esses trabalhos geralmente se concentram no uso de algoritmos de aprendizado de máquina ou minimização de critérios selecionados para otimizar os vários atributos que existem, como consequência, eles tendem a ter um alto custo computacional ao lidar com grandes quantidades de dados. Além disso, tais estudos abordam o desempenho da discretização através do prisma da classificação e alguns como forma de preprocessamento do conjunto de dados.

O método de discretização mais aplicado é baseado no espaçamento uniforme entre os pontos estimados. Ele trata igualmente todas as regiões de densidade (por exemplo, a cauda da função de densidade é discretizada com a mesma resolução de sua região de maior probabilidade) levando a um erro de estimativa que tende a não ser uniforme em todas as regiões da funções de densidade probabilidade.

\section{Avaliadores de Estimação}

Para avaliar um procedimento de estimação de \ac{PDF}, tornam-se necessárias medidas \textit{goodness-of-fit} que podem ser feitas considerando a distância ou a similaridade entre as funções.

Devido à sua importância, as medições de distância ou similaridade entre duas funções são fundamentais para melhorar a própria estimativa de \ac{PDF} e resolver problemas de classificação, \textit{clustering} e estimativa de densidade espectral. No entanto, existem muitas medidas de distância e similaridade na literatura, como pode ser visto em \cite{deza2006dictionary} e \cite{deza2009encyclopedia}, cada uma mais adequada para um tipo de aplicação do que para o outro.

Em \cite{basseville2013divergence}, foi escrita uma bibliografia anotada sobre medidas de divergência para processamento de dados estatísticos e problemas de inferência. No entanto, mesmo após muitos estudos, há uma demanda contínua para encontrar uma medida ideal de distância/similaridade entre funções para cada tipo de aplicação e suas particularidades.

Motivado pelos estudos apresentados em \cite{cha2007comprehensive}, que aborda o tema das muitas medidas de similaridade calculando a correlação entre as mesmas, os autores de \cite{souza2017study} tiveram como o principal objetivo encontrar uma medida eficiente de distância/similaridade a ser aplicada a métodos não-paramétricos de estimativa de densidade no contexto de classificação, caracterizando as principais medidas de distâncias/similaridade e avaliando suas sensibilidades para regiões da \ac{PDF} separadas.


