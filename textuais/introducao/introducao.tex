\chapter{INTRODUÇÃO} \label{cap:intro}
\vspace{-2cm}
A crescente evolução tecnológica vem possibilitando o desenvolvimento de muitas áreas do conhecimento, sendo uma delas a engenharia elétrica, mais precisamente a análise multivariada \cite{vicini2005analise} que torna-se cada vez mais uma ferramenta importante para a solução de problemas ligados à estimação de densidades e seleção de eventos, tanto no ambiente industrial quanto em laboratórios de pesquisa. Entretanto, tais problemas podem ocorrem em outras áreas do conhecimento, sendo assim, o esforço em prol da otimização dessas ferramentas de maneira multidisciplinar é de grande interesse.

Nas últimas décadas, a importância de uma modelagem estocástica por \ac{PDF}, utilizando-se de métodos não paramétricos teve um crescimento considerável devido ao fato de que vários experimentos geradores de enorme quantidade de dados foram iniciados.  Os experimentos ligados ao \ac{LHC} representam alguns deles. Desde a criação do \ac{CERN}, físicos e engenheiros de diferentes países têm trabalhado em conjunto para investigar questões referentes ao estado da arte da ciência fundamental relacionada à física de altas energias, usando instrumentos científicos complexos para estudar os constituintes básicos da matéria e suas interações. No complexo principal do \ac{CERN}, o \ac{LHC}, prótons são colocados em um acelerador que os faz colidir quase à velocidade da luz. Este processo permite estudar como as partículas interagem e fornece uma visão das leis fundamentais da natureza \cite{cernwebabout}.


Atualmente, a física experimental de altas energias é um ramo da ciência em progressiva expansão e pode ser considerada um dos campos científicos mais exigentes em termos de processamento de sinal, esse fato é explicado devido aos eventos de interesse serem raros e contaminados com alto nível de ruído de fundo, demandando sistemas cada vez mais otimizados no que diz respeito a tempo de processamento, eficiência de detecção de sinal e rejeição de ruído.

Com o objetivo de observar os subprodutos dessas colisões, é necessário usar detectores; basicamente, sensores que, trabalhando em conjunto, são capazes de medir algumas características dos subprodutos das colisões e transformá-los em sinais elétricos que podem ser armazenados e utilizados em estudos relacionados a física de altas energias.

Em geral, para problemas cujas variáveis podem ser modeladas, a estimação das mesmas se torna paramétrica. No entanto, é muito importante enfatizar que, devido à complexidade do problema, suas variáveis podem não ser descritas com as funções de densidade de probabilidade conhecidas na literatura. Sendo assim, a aplicação de métodos não paramétricos se espalhou consideravelmente nos últimos anos devido às ferramentas recentemente desenvolvidas para análise estatística. Tais métodos  fornecem um caminho alternativo a estimação paramétrica e possibilita o estudo de grandes quantidades de dados, essa linha de pesquisa torna-se objeto significativo de estudo, uma vez que contempla pesquisas teóricas e práticas com relação direta a temas como regressão, discriminação e reconhecimento de padrões. 

%Neste contexto, abre-se então a possibilidade de buscar uma combinação fundamentada entre estimação não paramétrica e processamento estatístico multivariado, no intuito de garantir uma resposta eficaz, robusta e viável.

Neste contexto, o presente trabalho visa avaliar os erros inseridos pelo processo de discretização propondo diferentes métodos e olhando diretamente à sua performance de estimação, considerando as interpolações pelo vizinho mais próximo e linear. O impacto pelos pontos longe da região de alta probabilidade também são avaliados uma vez que é um problema comum na estimação de \ac{PDF}.

\section{Motivação}

%Neste trabalho, serão propostos novos métodos de discretização a fim de reduzir os erros de estimação sem demandar de mais poder computacional para tal, tentando garantir uma quantidade de pontos adequadas a cada região de interesse.

A reconstrução e seleção de elétrons é de grande importância em muitas análises realizadas em experimentos de Física de Partículas, como é o caso do detector \ac{CMS} e do \ac{ATLAS} que utilizam o \ac{LHC}; o Belle \cite{hanagaki2002electron} que usa o KEK-B \cite{superk}; entre outros. No caso dos experimentos ATLAS e CMS a identificação correta dos elétrons é exigida para medições precisas do modelo padrão \cite{modelopadrao}, medições e pesquisas em busca do Bóson de Higgs, e pesquisas de processos que vão além do modelo padrão. Essas análises científicas exigem uma excelente resolução de momento e pequenas incertezas sistemáticas. É possível alcançar um alto nível de desempenho desses algoritmos fazendo uso de algumas etapas de processamento, evoluindo a partir dos algoritmos iniciais de reconstrução eletrônica desenvolvidos no contexto da seleção \textit{online} até algoritmos mais complexos no contexto de seleção \textit{offline}. Os princípios básicos da reconstrução de elétrons nesses detectores dependem de uma combinação da energia medida no calorímetro eletromagnético e o impulso medido no detector de traço.

Diversas estratégias podem ser usadas para identificar elétrons isolados (chamados de sinal), e separá-los de fontes de ruído de fundo, originários principalmente de conversões de fótons, jatos erroneamente identificados como elétrons, ou elétrons de decaimentos semi-leptônicos de quarks b e c. Algoritmos simples e robustos são desenvolvidos para aplicar seleções sequenciais em um conjunto discriminantes. Algoritmos mais complexos combinam variáveis em uma análise de \ac{MVA} para alcançar uma melhor discriminação. E uma das abordagens que tem ganhado força, nesse ambiente que requer cada vez mais precisão e desempenho, é o uso de métodos de classificação via probabilidade estatística, sendo que esses métodos tem como maior desafio a estimação de densidades que não podem ser parametrizadas pelas funções já conhecidas na literatura.

Portanto, na última década muitos trabalhos relacionados ao tema de otimização da estimação de densidade não paramétrica, tanto numérica \cite{schindler2012bandwidth} quanto computacional \cite{gramacki2017nonparametric}, foram publicados, bem como sobre o tema de discretização em processamento de sinais, mostrando que este é um tema que mesmo sendo discutido há décadas ainda é muito utilizado, explorado e em desenvolvimento. Portanto, abre-se a possibilidade de contribuir nessa área no que diz respeito a otimização da estimação de densidades usando como base de desenvolvimento um sistema altamente complexo e distribuições com características bastante distintas, como ocorre com os experimentos do \ac{LHC}. 

\section{O que foi feito}

Este trabalho se concentra na otimização da etapa de discretização do processo estimação de densidades não-paramétricas Abordando o problema do ponto de vista somente de estimação, avaliando o erro inserido nesse processo e buscando garantir uma otimização do \textit{trade-off} entre custo computacional e erro inserido. O desempenho dos algoritmos propostos foram avaliados em detalhe e comparados com outros métodos.

\section{Estrutura do Trabalho}

Este documento está organizado da seguinte maneira: o Capítulo~\ref{cap:disc} apresenta uma revisão bibliográfica do tema de discretização e introduz a matemática dos métodos que serão utilizados nesse trabalho. O Capítulo~\ref{cap:desenvolvimento} faz uma ambientação do meio onde está inserido esse trabalho e detalha o funcionamento do algoritmo de avaliação dos métodos propostos. O Capítulo~\ref{cap:resultados} traz os resultados utilizando-se os métodos propostos, as comparações e a analise do que foi pesquisado. Por fim, as conclusões e os próximos passos para a continuidade desse trabalho serão apresentados no Capítulo~\ref{cap:conclusao}.

